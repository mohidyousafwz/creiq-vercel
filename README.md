# CREIQ - Data Extraction Service

CREIQ (CRE Intelligence Query) is a Python-based web scraping application that extracts appeal data from the Assessment Review Board (ARB) website. It uses Playwright for browser automation and provides both API and command-line interfaces.

## ğŸš€ Features

- **Automated Data Extraction**: Extracts appeal information for multiple roll numbers
- **RESTful API**: Upload CSV files and process roll numbers via API endpoints
- **Property Information**: Captures property descriptions, classifications, and municipality data
- **Appeal Details**: Extracts appellant info, status, hearing details, and decisions
- **Batch Processing**: Handle multiple roll numbers efficiently
- **Background Processing**: Non-blocking API with task tracking
- **Test Mode**: Easy testing with single roll numbers

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- Node.js (for Playwright)
- Chrome/Chromium browser

## ğŸ› ï¸ Installation

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/creiq.git
cd creiq
```

2. **Create virtual environment**:
```bash
python -m venv myenv
# On Windows:
myenv\Scripts\activate
# On macOS/Linux:
source myenv/bin/activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Install Playwright browsers**:
```bash
playwright install chromium
```

5. **Set up environment variables**:
```bash
cp env.example .env
# Edit .env and add your ARB website URL
```

## ğŸš¦ Quick Start

### Testing Single Roll Number

```bash
python scripts/test_extraction.py
# Or with a specific roll number:
python scripts/test_extraction.py "38-29-300-012-10400-0000"
```

### Running the API Server

```bash
python main.py
# API will be available at http://localhost:8000
# Documentation at http://localhost:8000/docs
```

### Upload CSV via API

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@data/sample_upload_files/roll-number.csv"
```

## ğŸ“ Project Structure

```
CREIQ/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ creiq/
â”‚       â”œâ”€â”€ config/           # Configuration management
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ settings.py   # Application settings
â”‚       â”œâ”€â”€ models/           # Data models (future DB support)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ appeal.py     # Appeal data structures
â”‚       â”œâ”€â”€ services/         # Business logic
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ extraction_service.py
â”‚       â”œâ”€â”€ utils/            # Utility functions
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ logger.py
â”‚       â”‚   â””â”€â”€ roll_number_reader.py
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ api.py            # FastAPI application
â”‚       â””â”€â”€ playwright_automation.py  # Web scraping logic
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/                 # Unit tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_extraction.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_extraction.py    # Test single extraction
â”‚   â””â”€â”€ run_tests.py         # Run test suite
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_upload_files/  # Sample CSV files
â”‚   â”œâ”€â”€ results/              # Extraction results
â”‚   â””â”€â”€ test_extraction/      # Test outputs
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ main.py                   # Main entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ env.example              # Environment variables template
â”œâ”€â”€ docker-compose.yml       # Docker configuration
â””â”€â”€ README.md
```

## ğŸ“Š Output Format

The extraction creates a JSON file (`all_appeal_details.json`) with the following structure:

```json
{
  "roll_number": "38-29-300-012-10400-0000",
  "extracted_timestamp": "2025-06-11T15:02:35.689946",
  "page_title": "E-Services - Appeals",
  "property_info": {
    "description": "429 EXMOUTH ST PLAN 3 PT LOT 5 PLAN 96 LOT"
  },
  "appeals": [
    {
      "appeal_number": "1194369",
      "property_info": {
        "roll_number": "38-29-300-012-10400-0000",
        "municipality": "Sarnia City",
        "classification": "Commercial sport complexes",
        "nbhd": "293",
        "description": "429 EXMOUTH STPLAN 3 PT LOT 5 PLAN 96 LOT"
      },
      "appellant_info": {
        "name1": "J J W HOLDINGS LTD",
        "name2": "C/O DONALD STASIW",
        "representative": "D B BURNARD & ASSOCIATES",
        "filing_date": "31-March-2000",
        "tax_date": "01-January-2000",
        "section": "40",
        "reason_for_appeal": "Assessment Too High"
      },
      "status_info": {
        "status": "Closed"
      },
      "decision_info": {
        "decision_number": "1357206",
        "mailing_date": "23-June-2000",
        "decisions": "APPEAL WITHDRAWN (BEFORE SCHEDULING)",
        "decision_details": "HEARING # 17287."
      }
    }
  ]
}
```

## ğŸ§ª Testing

Run the test suite with coverage:

```bash
python scripts/run_tests.py
```

This will generate:
- Terminal coverage report
- HTML coverage report in `htmlcov/index.html`

## ğŸ”§ Configuration

Key environment variables in `.env`:

- `URL`: ARB website URL (required)
- `BROWSER_HEADLESS`: Run browser in headless mode (default: true)
- `API_HOST`: API host address (default: 0.0.0.0)
- `API_PORT`: API port number (default: 8000)
- `LOG_LEVEL`: Logging level (default: INFO)

See `env.example` for all available options.

## ğŸš§ Upcoming Features

- [ ] Web Dashboard with authentication
- [ ] Database integration (SQLAlchemy)
- [ ] Scheduled extraction (cron jobs)
- [ ] Export to multiple formats (Excel, PDF)
- [ ] Email notifications
- [ ] Docker deployment
- [ ] Rate limiting and retry logic
- [ ] Advanced filtering and search

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [Playwright](https://playwright.dev/) for reliable web automation
- [FastAPI](https://fastapi.tiangolo.com/) for the modern API framework
- Assessment Review Board for providing the public data
